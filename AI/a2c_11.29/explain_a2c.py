# explain_a2c.py

import os
import yaml
import numpy as np
import shap
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import joblib
import pandas as pd

from data_utils import (
    download_data,
    add_indicators,
    FEATURES,
    build_state,
)
from ac_model import A2CAgent


# --- feature ì´ë¦„ ìƒì„± (window_size ë°˜ì˜) ---
def get_feature_names_with_position(window_size: int):
    names = []
    for t in range(window_size):
        for f in FEATURES:
            names.append(f"{f}_t-{window_size-1-t}")
    names.append("Position")
    return names


# --- ì§€í‘œë³„ í•œê¸€ ì„¤ëª… ë§¤í•‘ ---
INDICATOR_DESC_KO = {
    "EMA12": "ë‹¨ê¸° ì¶”ì„¸(12ì¼ ì§€ìˆ˜ì´ë™í‰ê· )",
    "EMA26": "ì¤‘ê¸° ì¶”ì„¸(26ì¼ ì§€ìˆ˜ì´ë™í‰ê· )",
    "MACD": "ì¶”ì„¸ ëª¨ë©˜í…€(MACD ì§€í‘œ)",
    "Volume": "ê±°ë˜ëŸ‰ ë³€í™”",
    "KOSPI": "ì‹œì¥ ì „ì²´(KOSPI ì§€ìˆ˜) íë¦„",
    "SMA20": "ì¤‘ê¸° ì´ë™í‰ê· ì„ (20ì¼ ë‹¨ìˆœì´ë™í‰ê· )",
    "RSI": "ê³¼ë§¤ìˆ˜Â·ê³¼ë§¤ë„ ìˆ˜ì¤€(RSI)",
    "STOCH_%K": "ë‹¨ê¸° ëª¨ë©˜í…€(Stochastic %K)",
    "VIX": "ì‹œì¥ ë³€ë™ì„±(VIX ì§€ìˆ˜)",
    "BB_%B": "ë³¼ë¦°ì € ë°´ë“œ ë‚´ ìœ„ì¹˜(%B)",
    "BB_BW": "ë³¼ë¦°ì € ë°´ë“œ í­(ë³€ë™ì„±)",
    "ATR": "í‰ê·  ì§„í­(ATR, ë³€ë™ì„±)",
    "Position": "í˜„ì¬ í¬ì§€ì…˜(ë³´ìœ  ì—¬ë¶€)",
}


def calendar_split(df: pd.DataFrame, train_years: int, backtest_days: int):
    """
    train.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ 10ë…„ í•™ìŠµ + 1ë…„ ë°±í…ŒìŠ¤íŠ¸ êµ¬ê°„ì„ ë¶„í• .
    (ì—¬ê¸°ì„œëŠ” test_dfë§Œ ì‹¤ì œë¡œ ì‚¬ìš©)
    """
    last_date = df.index[-1]
    backtest_end = last_date
    backtest_start = backtest_end - pd.Timedelta(days=backtest_days)
    train_start = backtest_end - pd.DateOffset(years=train_years)

    train_df = df.loc[(df.index >= train_start) & (df.index < backtest_start)].copy()
    test_df = df.loc[(df.index >= backtest_start) & (df.index <= backtest_end)].copy()

    print("\n[ë°ì´í„° ë¶„í• (ìº˜ë¦°ë” ê¸°ì¤€, explainìš©)]")
    print(
        f"  - Train ê¸°ê°„: {train_df.index[0].date()} ~ {train_df.index[-1].date()} "
        f"({len(train_df)}ì¼)"
    )
    print(
        f"  - Test  ê¸°ê°„: {test_df.index[0].date()} ~ {test_df.index[-1].date()} "
        f"({len(test_df)}ì¼)"
    )
    return train_df, test_df


# --- ì¶”ì²œ + SHAP ê³„ì‚° ---
def get_recommendation_and_explanation(
    state: np.ndarray,
    agent: A2CAgent,
    explainer: shap.Explainer,
    feature_names: list,
    save_path: str,
    top_k: int = 3,
):
    # 1. ì •ì±… í™•ë¥  ê³„ì‚°
    with torch.no_grad():
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        policy_logits, _ = agent.ac_net(state_tensor)
        policy_probs = F.softmax(policy_logits, dim=-1).detach().cpu().numpy()[0]

    action_idx = int(np.argmax(policy_probs))
    action_map = {0: "ë§¤ìˆ˜", 1: "ë§¤ë„", 2: "ë³´ìœ "}
    action_label = action_map[action_idx]
    recommendation = action_label  # ìì—°ì–´ìš©

    # 2. SHAP ê°’ ê³„ì‚°
    print("SHAP ê°’ ê³„ì‚° ì¤‘ (A2C Actor ê¸°ì¤€)...")
    shap_values_tensor = explainer.shap_values(state.reshape(1, -1))
    shap_for_action = shap_values_tensor[0, :, action_idx]

    # 3. featureë³„ SHAP dict
    feature_shap = dict(zip(feature_names, shap_for_action))

    # 3-1. base feature ë‹¨ìœ„ë¡œ ë¬¶ê¸°(EMA12, VIX ë“±)
    aggregate = {}
    for name, val in feature_shap.items():
        if "_t-" in name:
            base = name.split("_t-")[0]
        else:
            base = name
        if base not in aggregate:
            aggregate[base] = {"sum_abs": 0.0, "best_val": val}
        aggregate[base]["sum_abs"] += abs(val)
        if abs(val) > abs(aggregate[base]["best_val"]):
            aggregate[base]["best_val"] = val

    # 3-2. ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_base = sorted(
        aggregate.items(), key=lambda x: x[1]["sum_abs"], reverse=True
    )

    top_features = []
    for base, info in sorted_base[:top_k]:
        shap_val = info["best_val"]
        direction = "ì§€ì§€" if shap_val > 0 else "ë°©í•´"
        desc = INDICATOR_DESC_KO.get(base, base)
        top_features.append(
            {
                "base": base,
                "shap": shap_val,
                "direction": direction,
                "description": desc,
            }
        )

    # 4. ì „ì²´ feature ë‹¨ìœ„ barh ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(10, 6))
    order = np.argsort(shap_for_action)
    plt.barh([feature_names[i] for i in order], shap_for_action[order])
    plt.title(f"'{recommendation}' ì¶”ì²œì— ëŒ€í•œ SHAP ê¸°ì—¬ë„ (A2C Actor)")
    plt.xlabel("SHAP Value (ì •ì±… í™•ë¥  ê¸°ì—¬ë„: +ëŠ” ì§€ì§€, -ëŠ” ë°©í•´)")
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.show()
    plt.close()

    return recommendation, policy_probs, feature_shap, top_features


def run_prediction():
    print("A2C ì˜ˆì¸¡ ë° SHAP ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1. ì„¤ì • ë¡œë“œ
    with open("config.yaml", "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    report_dir = cfg["report_dir"]
    scaler_path = os.path.join(report_dir, "scaler.joblib")
    model_path = cfg["model_path"]
    window_size = cfg["window_size"]
    model_cfg = cfg["model_cfg"]
    train_years = cfg.get("train_years", 10)
    backtest_days = cfg.get("backtest_days", 365)

    # 2. ë°ì´í„° ë¡œë“œ
    raw = download_data(
        cfg["ticker"],
        cfg["kospi_ticker"],
        cfg["vix_ticker"],
        cfg["start_date"],
        cfg["end_date"],
    )
    df = add_indicators(raw)

    # train/test ë¶„í•  (trainì€ ë°°ê²½ ë°ì´í„°ìš©, testëŠ” ëŒ€ìƒ window ì„ íƒìš©)
    train_df, test_df = calendar_split(df, train_years, backtest_days)

    # 3. ìŠ¤ì¼€ì¼ëŸ¬ & ëª¨ë¸ ë¡œë“œ
    try:
        scaler = joblib.load(scaler_path)
        print(f"[OK] Scaler ë¡œë“œ: {scaler_path}")
    except FileNotFoundError:
        print(f"[ì—ëŸ¬] {scaler_path} ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•´ì„œ í•™ìŠµí•˜ì„¸ìš”.")
        return

    train_df[FEATURES] = scaler.transform(train_df[FEATURES])
    test_df[FEATURES] = scaler.transform(test_df[FEATURES])

    print(f"[OK] A2C ëª¨ë¸ ë¡œë“œ: {model_path}")
    dummy_state_dim = len(FEATURES) * window_size + 1

    agent = A2CAgent(
        state_dim=dummy_state_dim,
        action_dim=3,
        hidden_dims=model_cfg.get("hidden_dims", [128, 128]),
        gamma=cfg["gamma"],
        lr=cfg["lr"],
        value_loss_coeff=cfg["value_loss_coeff"],
        entropy_coeff=cfg["entropy_coeff"],
        seed=cfg["seed"],
        device=cfg.get("device", "cpu"),
    )
    agent.load(model_path)

    # 4. SHAPìš© ë°°ê²½ ë°ì´í„°
    print("SHAP ë°°ê²½ ë°ì´í„°(í›ˆë ¨ ìƒ˜í”Œ ì¼ë¶€) ìƒì„± ì¤‘...")
    bg_states = []
    bg_len = min(200 + window_size, len(train_df) - 1)
    for i in range(window_size - 1, bg_len):
        current_window = train_df.iloc[i - (window_size - 1) : i + 1]
        s = build_state(current_window, position_flag=0)
        bg_states.append(s)
    bg_states = np.array(bg_states, dtype=np.float32)
    bg_summary = shap.sample(bg_states, 100)

    def model_f(x):
        x_t = torch.tensor(x, dtype=torch.float32, device=cfg.get("device", "cpu"))
        policy_logits, _ = agent.ac_net(x_t)
        policy_probs = F.softmax(policy_logits, dim=-1)
        return policy_probs.detach().cpu().numpy()

    print("SHAP KernelExplainer ìƒì„± ì¤‘...")
    explainer = shap.KernelExplainer(model_f, bg_summary)
    feature_names = get_feature_names_with_position(window_size)
    print("[OK] SHAP ì¤€ë¹„ ì™„ë£Œ")

    # 5. ì˜ˆì¸¡ ëŒ€ìƒ: í…ŒìŠ¤íŠ¸ ë§ˆì§€ë§‰ ë‚  ê¸°ì¤€ (window_sizeë§Œí¼ ê³¼ê±° í¬í•¨)
    target_idx = len(test_df) - 2  # ë§ˆì§€ë§‰ ë°”ë¡œ ì „ë‚ ì„ ê¸°ì¤€ìœ¼ë¡œ
    target_window = test_df.iloc[target_idx - (window_size - 1) : target_idx + 1]
    target_date = test_df.index[target_idx]

    current_position = 0  # ì›¹ ì„œë¹„ìŠ¤ ê¸°ì¤€: ê¸°ë³¸ê°’ì€ ë¯¸ë³´ìœ 
    state_to_explain = build_state(target_window, current_position)

    # 6. ì¶”ì²œ + ì„¤ëª…
    save_path = os.path.join(report_dir, "prediction_explanation_a2c.png")
    print(f"\nê¸°ì¤€ ë‚ ì§œ: {target_date.strftime('%Y-%m-%d')}")
    print(f"SHAP ê·¸ë˜í”„ ì €ì¥ ìœ„ì¹˜: {save_path}")

    rec, probs, shap_all, top_feats = get_recommendation_and_explanation(
        state_to_explain, agent, explainer, feature_names, save_path
    )

    # critic value (DQNì˜ Q-value ëŒ€ì‹ , ìƒíƒœ ê°€ì¹˜ V(s) ì‚¬ìš©)
    with torch.no_grad():
        v_pred = agent.get_value(state_to_explain).item()

    # 7. íŒ€ì› ìŠ¤íƒ€ì¼ì˜ ì¶œë ¥
    print("\n=============================================")
    print(f"      [ ğŸ“± ë¦¬ë¸Œë¦¬ AI ë¶„ì„ ê²°ê³¼ ({cfg['ticker']}) ]")
    print("=============================================\n")

    print("--- 1. AI ìµœì¢… ì‹ í˜¸ ---")
    print(f"    {rec}")
    print(f"    (ì˜ˆìƒ íŒ€ Score(Vê°’): {v_pred:.4f})")

    print("\n--- 2. AI ì„¤ëª… ---")
    print(f"AIê°€ '{rec}'ì„(ë¥¼) ê²°ì •í•œ ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n")

    # TOP3 ì„¤ëª… ë¬¸ì¥
    for i, feat in enumerate(top_feats, start=1):
        base = feat["base"]
        desc = feat["description"]
        if i == 1:
            tail = "ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤."
        elif i == 2:
            tail = "2ìˆœìœ„ë¡œ ê²°ì •ì— ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
        else:
            tail = "ë§ˆì§€ë§‰ìœ¼ë¡œ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤."
        print(f"  {i}. '{base}' ì§€í‘œ({desc})ì˜ ìµœê·¼ ì›€ì§ì„ì„ {tail}")

    # (ì„ íƒ) ì •ì±… í™•ë¥ ë„ ê°™ì´ ë³´ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ìœ ì§€
    print("\n--- ì •ì±… í™•ë¥  (Policy Probabilities) ---")
    action_labels = ["ë§¤ìˆ˜", "ë§¤ë„", "ë³´ìœ "]
    for i, p in enumerate(probs):
        print(f"  {action_labels[i]:<4}: {p:.4f}")


if __name__ == "__main__":
    run_prediction()