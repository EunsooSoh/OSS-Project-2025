# evaluate_a2c.py

import os
import yaml
import joblib
import numpy as np
import pandas as pd
import shap
import torch
import torch.nn.functional as F

from sklearn.preprocessing import StandardScaler

from data_utils import (
    download_data,
    add_indicators,
    FEATURES,
    build_state,
)
from trading_env import TradingEnv
from ac_model import A2CAgent


def pretty_feature_name(feat_name: str) -> str:
    """SHAP feature ì´ë¦„ì„ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ ë³€í™˜."""
    base = feat_name.split("_t-")[0]
    mapping = {
        "EMA12": "ë‹¨ê¸° ì¶”ì„¸(EMA12)",
        "EMA26": "ì¤‘ê¸° ì¶”ì„¸(EMA26)",
        "MACD": "MACD(ì¶”ì„¸ ëª¨ë©˜í…€)",
        "Volume": "ê±°ë˜ëŸ‰",
        "KOSPI": "ì½”ìŠ¤í”¼ ì§€ìˆ˜",
        "SMA20": "20ì¼ ì´ë™í‰ê· ì„ ",
        "RSI": "RSI(ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„)",
        "STOCH_%K": "ìŠ¤í† ìºìŠ¤í‹± %K",
        "VIX": "ë³€ë™ì„± ì§€ìˆ˜(VIX)",
        "BB_%B": "ë³¼ë¦°ì € %b",
        "BB_BW": "ë³¼ë¦°ì € ë°´ë“œ í­",
        "ATR": "ATR(ë³€ë™ì„±)",
        "Position": "í˜„ì¬ í¬ì§€ì…˜",
    }
    return mapping.get(base, base)


def get_feature_names_with_position(window_size: int):
    names = []
    for t in range(window_size):
        for f in FEATURES:
            names.append(f"{f}_t-{window_size-1-t}")
    names.append("Position")
    return names


def compute_backtest_metrics(daily_returns: np.ndarray):
    """ì¼ë³„ ìˆ˜ìµë¥  ë°°ì—´ë¡œë¶€í„° ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°."""
    n = len(daily_returns)
    if n == 0:
        return None

    # ëˆ„ì  ìˆ˜ìµ (1 -> equity ë§ˆì§€ë§‰ - 1)
    equity = np.cumprod(1.0 + daily_returns)
    cum_return = equity[-1] - 1.0

    mean_ret = daily_returns.mean()
    vol = daily_returns.std(ddof=1) if n > 1 else 0.0
    sharpe = (mean_ret / vol * np.sqrt(252)) if vol > 0 else np.nan

    win_mask = daily_returns > 0
    wins = int(win_mask.sum())
    win_rate = wins / n if n > 0 else 0.0

    # ìµœëŒ€ ë‚™í­(MDD)
    running_max = np.maximum.accumulate(equity)
    drawdown = equity / running_max - 1.0
    mdd = drawdown.min() if len(drawdown) > 0 else 0.0

    return {
        "days": n,
        "cum_return": cum_return,
        "mean_ret": mean_ret,
        "vol": vol,
        "sharpe": sharpe,
        "wins": wins,
        "win_rate": win_rate,
        "mdd": mdd,
    }


def run_evaluation_and_explain():
    print("A2C ë°±í…ŒìŠ¤íŠ¸ ë° ìµœì¢…ì¼ ì„¤ëª…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1. ì„¤ì • ë¡œë“œ
    with open("config.yaml", "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    report_dir = cfg["report_dir"]
    scaler_path = os.path.join(report_dir, "scaler.joblib")
    model_path = cfg["model_path"]
    window_size = cfg["window_size"]
    model_cfg = cfg["model_cfg"]

    # --- (ì„ íƒ) í•™ìŠµ ê³¡ì„  ë¶„ì„: train_history npz ì½ê¸° ---
    hist_path = os.path.join(report_dir, "a2c_train_history.npz")
    print("\n--- í•™ìŠµ ê³¡ì„  ë¶„ì„ ---")
    if os.path.exists(hist_path):
        hist = np.load(hist_path)
        ep_rewards = hist["episode_rewards"]
        val_rewards = hist["val_rewards"] if "val_rewards" in hist.files else np.array([])

        first_100_mean = ep_rewards[:100].mean() if len(ep_rewards) >= 100 else ep_rewards.mean()
        last_100_mean = ep_rewards[-100:].mean() if len(ep_rewards) >= 100 else ep_rewards.mean()
        best_ep_reward = ep_rewards.max()
        best_val = val_rewards.max() if len(val_rewards) > 0 else np.nan

        print(f"    - ì´ˆê¸° 100 ì—í”¼ì†Œë“œ í‰ê· : {first_100_mean:.2f}")
        print(f"    - ìµœì¢… 100 ì—í”¼ì†Œë“œ í‰ê· : {last_100_mean:.2f}")
        print(f"    - ìµœê³  ì—í”¼ì†Œë“œ ë³´ìƒ: {best_ep_reward:.2f}")
        print(f"    - ìµœê³  ê²€ì¦ ë³´ìƒ: {best_val:.2f}")
    else:
        print("    (ê²½ê³ ) a2c_train_history.npz íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("    -> ìƒˆ train.pyë¡œ ë‹¤ì‹œ í•™ìŠµí•˜ë©´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.")

    # 2. ë°ì´í„° ë¡œë“œ (config ë²”ìœ„ ì „ì²´)
    raw = download_data(
        cfg["ticker"],
        cfg["kospi_ticker"],
        cfg["vix_ticker"],
        cfg["start_date"],
        cfg["end_date"],
    )
    df = add_indicators(raw)

    # ë§ˆì§€ë§‰ 365 ê±°ë˜ì¼ì„ ë°±í…ŒìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    backtest_days = 365
    if len(df) <= backtest_days + window_size:
        raise ValueError("ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ì•„ì„œ 365ì¼ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”.")

    test_df = df.iloc[-backtest_days:].copy()
    train_df = df.iloc[:-backtest_days].copy()

    backtest_start = test_df.index[0].date()
    backtest_end = test_df.index[-1].date()

    # 3. ìŠ¤ì¼€ì¼ëŸ¬ ë° ëª¨ë¸ ë¡œë“œ
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"{scaler_path} ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. train.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    scaler: StandardScaler = joblib.load(scaler_path)

    train_df[FEATURES] = scaler.transform(train_df[FEATURES])
    test_df[FEATURES] = scaler.transform(test_df[FEATURES])

    print(f"\nScaler ë¡œë“œ ì™„ë£Œ: {scaler_path}")
    print(f"í•™ìŠµëœ A2C ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

    state_dim = len(FEATURES) * window_size + 1
    agent = A2CAgent(
        state_dim=state_dim,
        action_dim=3,
        hidden_dims=model_cfg.get("hidden_dims", [128, 128]),
        gamma=cfg["gamma"],
        lr=cfg["lr"],
        value_loss_coeff=cfg["value_loss_coeff"],
        entropy_coeff=cfg["entropy_coeff"],
        seed=cfg["seed"],
        device=cfg.get("device", "cpu"),
    )
    agent.load(model_path)

    # 4. ë°±í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„±
    env_cfg = {
        "data": test_df,
        "window_size": window_size,
        "trade_penalty": cfg["trade_penalty"],
        "use_daily_unrealized": cfg["use_daily_unrealized"],
        "reward_cfg": cfg.get("reward", {}),
    }
    env = TradingEnv(**env_cfg)

    # 5. ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (greedy ì •ì±…)
    s = env.reset()
    done = False
    rets = []
    actions = []

    print("\n--- [1] ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë°±í…ŒìŠ¤íŠ¸ ---")
    while not done:
        a, _ = agent.act(s, deterministic=True)
        ns, r, done, info = env.step(a)
        rets.append(r)      # ì—¬ê¸°ì„œëŠ” rì„ 'ì¼ë³„ ìˆ˜ìµë¥ 'ë¡œ ê°„ì£¼
        actions.append(a)
        s = ns if not done else s

    rets = np.array(rets, dtype=float)
    actions = np.array(actions, dtype=int)

    # 6. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    metrics = compute_backtest_metrics(rets)

    print("\n--- [2] ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì§€í‘œ ---")
    if metrics is None:
        print("    (ì˜¤ë¥˜) ìˆ˜ìµë¥  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„: {metrics['days']} ì¼ ({backtest_start} ~ {backtest_end})")
    print(f"    - ëˆ„ì  ìˆ˜ìµ: {metrics['cum_return']:.2f}")
    print(f"    - ì¼ í‰ê·  ìˆ˜ìµ: {metrics['mean_ret']:.4f}")
    print(f"    - ì¼ ìˆ˜ìµ ë³€ë™ì„±: {metrics['vol']:.4f}")
    print(f"    - ìƒ¤í”„ ë¹„ìœ¨ (ì—°í™˜ì‚°): {metrics['sharpe']:.3f}")
    print(f"    - ìŠ¹ë¥ : {metrics['win_rate']*100:.2f}% ({metrics['wins']}/{metrics['days']} ì¼)")
    print(f"    - ìµœëŒ€ ë‚™í­(MDD): {metrics['mdd']:.2f}")

    # í–‰ë™ ë¶„í¬ (ë‹¨ì¼ ì—ì´ì „íŠ¸ìš©)
    buy_pct = (actions == 0).mean() * 100.0  # Long
    sell_pct = (actions == 1).mean() * 100.0 # Short
    hold_pct = (actions == 2).mean() * 100.0 # Hold

    print("\n    - í–‰ë™ ë¶„í¬:")
    print(f"      Agent: Buy={buy_pct:.1f}% Hold={hold_pct:.1f}% Sell={sell_pct:.1f}%")

    # 7. ìµœì¢…ì¼ SHAP ë¶„ì„
    #    - train_dfì—ì„œ background sample ìƒì„±
    print("\n--- [3] ìµœì¢…ì¼ ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„ ---")

    bg_states = []
    bg_len = min(200 + window_size, len(train_df) - 1)
    for i in range(window_size - 1, bg_len):
        window = train_df.iloc[i - (window_size - 1) : i + 1]
        state_bg = build_state(window, position_flag=0)
        bg_states.append(state_bg)
    bg_states = np.array(bg_states, dtype=np.float32)
    bg_summary = shap.sample(bg_states, 100)

    device = cfg.get("device", "cpu")

    def model_f(x):
        x_t = torch.tensor(x, dtype=torch.float32, device=device)
        logits, _ = agent.ac_net(x_t)
        probs = F.softmax(logits, dim=-1)
        return probs.detach().cpu().numpy()

    explainer = shap.KernelExplainer(model_f, bg_summary)

    # ìµœì¢…ì¼ ê¸°ì¤€ window ìƒíƒœ ìƒì„±
    target_window = test_df.iloc[-window_size:]
    target_date = target_window.index[-1].date()
    state_last = build_state(target_window, position_flag=0)

    shap_values = explainer.shap_values(state_last.reshape(1, -1))
    # KernelExplainer ë°˜í™˜ì´ (N, state_dim, action_dim) í˜•íƒœë¼ê³  ê°€ì •
    # shap_values[0] -> (state_dim, action_dim)
    shap_tensor = shap_values[0]
    # ìµœì¢… í–‰ë™: greedy ì •ì±…
    with torch.no_grad():
        st = torch.tensor(state_last, dtype=torch.float32).unsqueeze(0)
        logits, _ = agent.ac_net(st)
        probs = F.softmax(logits, dim=-1).detach().cpu().numpy()[0]

    action_idx = int(np.argmax(probs))
    action_map = {0: "ë§¤ìˆ˜", 1: "ë§¤ë„", 2: "ë³´ìœ "}
    action_name = action_map[action_idx]

    shap_for_action = shap_tensor[:, action_idx]
    feature_names = get_feature_names_with_position(window_size)
    explanation = dict(zip(feature_names, shap_for_action))

    # ì¤‘ìš”ë„ ìƒìœ„ 3ê°œ (ì ˆëŒ“ê°’ ê¸°ì¤€)
    top3 = sorted(explanation.items(), key=lambda kv: abs(kv[1]), reverse=True)[:3]

    print("\n=============================================")
    print("      [ ğŸ“± ë¦¬ë¸Œë¦¬ AI ë¶„ì„ ê²°ê³¼ (ì‚¼ì„±ì „ì) ]")
    print("=============================================\n")

    print("--- 1. AI ìµœì¢… ì‹ í˜¸ ---")
    print(f"    {action_name}")
    print(f"    (ì •ì±… í™•ë¥ : Long={probs[0]:.4f}, Short={probs[1]:.4f}, Hold={probs[2]:.4f})")

    print("\n--- 2. AI ì„¤ëª… ---")
    print(f"AIê°€ '{action_name}'ì„(ë¥¼) ê²°ì •í•œ ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n")
    for rank, (fname, val) in enumerate(top3, start=1):
        pretty = pretty_feature_name(fname)
        direction = "ê¸ì •ì (+)" if val > 0 else "ë¶€ì •ì (-)"
        print(f"  {rank}. '{pretty}' ì§€í‘œì˜ ì˜í–¥ì´ {val:+.4f} ({direction}) ë¡œ í¬ê²Œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤.")

    print(f"\n(ê¸°ì¤€ ë‚ ì§œ: {target_date})")


if __name__ == "__main__":
    run_evaluation_and_explain()